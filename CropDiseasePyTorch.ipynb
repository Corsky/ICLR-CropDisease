{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhilin Guo (zg2358) & Fangpu He (fh2398)  \n",
    "4701 AI Kaggle Competition**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess\n",
    "fileRoot = \"D:\\\\ICLR-CropDisease\\\\dataset\\\\\"\n",
    "\n",
    "def loadData():\n",
    "    print(\"loadData called\")\n",
    "    \n",
    "    data_img = []\n",
    "    data_label = []\n",
    "\n",
    "    for file in os.listdir(fileRoot + \"train\\\\healthy_wheat\\\\\"):\n",
    "        img = cv2.imread(fileRoot + \"train\\\\healthy_wheat\\\\\" + file)\n",
    "        res = cv2.resize(img, dsize=(128, 128))\n",
    "        data_img.append(res)\n",
    "        data_label.append(0)\n",
    "\n",
    "    for file in os.listdir(fileRoot + \"train\\\\leaf_rust\\\\\"):\n",
    "        img = cv2.imread(fileRoot + \"train\\\\leaf_rust\\\\\" + file)\n",
    "        if img is None:\n",
    "            print(file)\n",
    "            continue\n",
    "        res = cv2.resize(img, dsize=(128, 128))\n",
    "        data_img.append(res)\n",
    "        data_label.append(1)\n",
    "\n",
    "    for file in os.listdir(fileRoot + \"train\\\\stem_rust\\\\\"):\n",
    "        img = cv2.imread(fileRoot + \"train\\\\stem_rust\\\\\" + file)\n",
    "        res = cv2.resize(img, dsize=(128, 128))\n",
    "        data_img.append(res)\n",
    "        data_label.append(2)\n",
    "\n",
    "    for i in range(len(data_img)):\n",
    "        data_img[i] = data_img[i] / 255\n",
    "    data_img = np.array(data_img)\n",
    "    return data_img, data_label\n",
    "\n",
    "\n",
    "def trainTestSplit(data_img,data_label):\n",
    "    print(\"trainTestSplit called\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_img,data_label,test_size = 0.3)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#Create CNN model\n",
    "# Current : 3 conv layers, 2 pooling, 1 flatten, 2 dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadData called\n",
      "7U06EV.gif\n",
      "trainTestSplit called\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "data_img,data_lable = loadData()\n",
    "\n",
    "x_train, x_test, y_train, y_test = trainTestSplit(data_img,data_lable)\n",
    "y_train =np.array(y_train)\n",
    "y_test =np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quirk of keras; since the images are grayscale,\n",
    "# we need to add an axis so the shape is (60000, 28, 28, 1)\n",
    "# instead of (60000, 28, 28)\n",
    "\n",
    "#x_train = x_train[:,:,:,np.newaxis]\n",
    "#x_test = x_test[:,:,:,np.newaxis]\n",
    "\n",
    "# We're also going to convert 0~255 to 0~1 float.\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Finally, the classes need to be one-hot encoded.\n",
    "# That is:\n",
    "# 0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# etc.\n",
    "# This is to match what the network will output - \n",
    "# there are 10 nodes at the end, each with its own\n",
    "# confidence of its class. The ground truth should be\n",
    "# 100% confidence of the true label.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation\n",
    "\n",
    "# By the way, we really like powers of 2 for the number\n",
    "# of nodes at each layer.\n",
    "\n",
    "# model = Sequential([\n",
    "#     # input layer, 16 conv (spatial) perceptrons of size (3,3)\n",
    "#     # image shape is (28, 28, 1). If it was color it'd be (28, 28, 3)\n",
    "#     Conv2D(30, (5,5), activation='relu', input_shape=(28, 28, 1)),\n",
    "#     # Now for the max pooling to make the size smaller\n",
    "#     MaxPooling2D(pool_size=(2,2)),\n",
    "#     # Flatten before sending to Dense (2D to 1D)\n",
    "#     Flatten(),\n",
    "#     # Output layer with 10 nodes for 10 classes, with softmax\n",
    "#     Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "#our own model with sequential layers\n",
    "model = Sequential([\n",
    "    # input layer, 16 conv (spatial) perceptrons of size (3,3)\n",
    "    # image shape is (28, 28, 1). If it was color it'd be (28, 28, 3)\n",
    "    Conv2D(64, kernel_size=(6,6), activation='relu', input_shape=(28, 28, 1)),\n",
    "    # Now for the max pooling to make the size smaller\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    # another Conv2D layer\n",
    "    Conv2D(32, (5,5), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    \n",
    "    # prevent overfitting\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    # Flatten before sending to Dense\n",
    "    Flatten(),\n",
    "    \n",
    "    # multiple dense layers\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Output layer with 10 nodes for 10 classes, with softmax\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.SGD(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_3_input to have 4 dimensions, but got array with shape (612, 128, 128, 1, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-04f5810088f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m      \u001b[1;31m# print progress in console\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# validation data to check generalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m           epochs= 1000)       # how many times to go through the entire training set\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training took\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"seconds.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_3_input to have 4 dimensions, but got array with shape (612, 128, 128, 1, 3)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model.fit(x_train,        # training data\n",
    "          y_train,        # training labels\n",
    "          batch_size=300,  # how many training examples you want to give at once\n",
    "          verbose=1,      # print progress in console\n",
    "          validation_data=(x_test, y_test),  # validation data to check generalization\n",
    "          epochs= 1000)       # how many times to go through the entire training set\n",
    "end = time.time()\n",
    "print(\"Training took\", end-start, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "scan_test_x = np.load(dir_path+'scan-test-images.npy')\n",
    "scan_test_x = scan_test_x[:,:,:,np.newaxis]\n",
    "scan_test_x = scan_test_x.astype(np.float)\n",
    "scan_test_x /= 255\n",
    "\n",
    "with open('output.csv', mode='w') as csv_file:\n",
    "    res = model.predict(scan_test_x)\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"Id\", \"Category\"])\n",
    "    id = 0\n",
    "    for pred_res in res:\n",
    "        pred_num = np.argmax(pred_res)\n",
    "        csv_writer.writerow([id, pred_num])\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_test_x = np.load(dir_path+'scan-test-images.npy')\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "# show each image, and make each title the label\n",
    "# these are grayscale images so use appropriate heatmap\n",
    "ax1.imshow(scan_test_x[0], cmap=plt.get_cmap('gray'))\n",
    "ax2.imshow(scan_test_x[1], cmap=plt.get_cmap('gray'))\n",
    "ax3.imshow(scan_test_x[2], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
